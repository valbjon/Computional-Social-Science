{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import timeit\n",
    "import requests\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This crawler crawls all the articles that were nominated for deletion but didn't get deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The crawler is a one-shot crawler. \n",
    "- For wikipedia articles from 2006 and on it is crawling roughly 98% of the data. \n",
    "- For wikipedia articles below 2006 (the HTML structure changes) it is crawling roughly 90% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_year(year, yearContent, df):\n",
    "    \"\"\" Crawl the different years of the wikipedia's archieved deletion discussions page and store the content\n",
    "        in a Data Frame. \n",
    "\n",
    "        Args: \n",
    "            year: the year in which the archived articles were flagged for deletion\n",
    "            yearContent: the html content containing the year (a h2 tag)\n",
    "            df: the data frame where the data are stored in the format year | month | title | Id | Gender\n",
    "\n",
    "        Returns:\n",
    "            a data frame\n",
    "    \"\"\"\n",
    "    for monthContent in yearContent.find_next_siblings(limit=24):\n",
    "        if monthContent.name == \"h2\":\n",
    "            # Crawl only this year. If the year doesn't yet have 12 months(e.g. 2019), don't go for more.\n",
    "            break\n",
    "        elif monthContent.name == \"h3\":\n",
    "            month = monthContent.get_text().split(str(year)+\" \")[1].split(\"[\")[0]\n",
    "            print(\"Month\",month)\n",
    "        elif monthContent.name == \"ul\":\n",
    "            # Go through the list of days\n",
    "            for dayRelative in monthContent.find_all(\"a\"):\n",
    "                print(dayRelative['href'])\n",
    "                dayPageLink = \"https://en.wikipedia.org/\"+dayRelative['href']\n",
    "                try :\n",
    "                    dayPage = requests.get(dayPageLink)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    continue\n",
    "                soupPage = BeautifulSoup(dayPage.content, \"html.parser\")\n",
    "\n",
    "                if dayPage.status_code == 200:\n",
    "                    \n",
    "                    # Get the number of articles in a particular day\n",
    "                    # From the beginning till the june 2006 wikipedia has a different HTML code on this\n",
    "                    if (int(year) < 2006 | ((int(year) == 2006) & (month in ['June', 'May', 'April', 'March', 'February', \n",
    "                                                                          'January']))):\n",
    "                        try:\n",
    "                            articlesLength = float(soupPage.find_all(\"li\", {\"class\": \"toclevel-2\"})[-1].get_text().split(\" \")[0])\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        nrLength = len(str(articlesLength).split(\".\")[1])\n",
    "                        if nrLength == 2:\n",
    "                            articlesLength = round(articlesLength%1 * 100,2)\n",
    "                        elif nrLength == 3:\n",
    "                            articlesLength = round(articlesLength%1 * 1000,3)\n",
    "                    else:\n",
    "                        try:\n",
    "                            articlesLength = float(soupPage.find_all(\"ul\")[2].find_all(\"li\")[-1].get_text().split(\" \")[0])\n",
    "                        except ValueError:\n",
    "                            try:\n",
    "                                articlesLength = float(soupPage.find_all(\"ul\")[0].find_all(\"li\")[-1].get_text().split(\" \")[0])\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                        numberDec = round(articlesLength % 1 * 10, 2)\n",
    "                        if int(numberDec) != numberDec:\n",
    "                            numberDec *= 10\n",
    "                        articlesLength = int(articlesLength) + numberDec\n",
    "                    \n",
    "                    print(\"Articles to be crawled in this page: \",articlesLength)\n",
    "\n",
    "                    # Every article is located in an <h3> tag\n",
    "                    for article in soupPage.find_all(\"h3\", limit = articlesLength):\n",
    "                        try:\n",
    "                            # Don't read deleted articles\n",
    "                            if article.find(\"a\")['title'].find(\"(page does\") == -1:\n",
    "                                articleTitle = article.get_text().split(\"[\")[0]\n",
    "                                pageLink = \"https://en.wikipedia.org\"+article.find(\"a\")['href']\n",
    "                                df = crawl_article(year, month, articleTitle, pageLink, df)\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_article(year, month, title, pageLink, df):\n",
    "    \"\"\" Crawl the content of the corresponding dbpedia page of a wikipedia article in order to get its id and gender.\n",
    "        Store an entry in the dataframe.\n",
    "\n",
    "        Args:\n",
    "            year: the year in which the current article was flagged for deletion\n",
    "            month: the month in which the current article was flagged for deletion\n",
    "            articleTitle: the title of the article flagged for deletion\n",
    "            pageLink: the wikipedia link of the article\n",
    "            df: the data frame where the data are stored in the format year | month | title | Id | gender\n",
    "\n",
    "        Returns:\n",
    "            A data frame\n",
    "    \"\"\"\n",
    "    url = \"http://dbpedia.org/page/\"+pageLink.split(\"/wiki/\")[1]\n",
    "    try :\n",
    "        dbpediaPage = requests.get(url)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return df\n",
    "    soup = BeautifulSoup(dbpediaPage.content, \"html.parser\")\n",
    "    wikiIdTag = soup.find(\"span\", {\"property\":\"dbo:wikiPageID\"})\n",
    "    genderTag = soup.find(\"span\", {\"property\":\"foaf:gender\"})\n",
    "    if genderTag == None:\n",
    "        # Not a person\n",
    "        return df\n",
    "    dic = {\"Year\":year, \"Month\":month, \"Tile\":title, \"Id\": wikiIdTag.contents[0]\n",
    "           , \"Gender\":genderTag.contents[0]}\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(data=dic, index=[0])\n",
    "    else:\n",
    "        df_temp = pd.DataFrame(data=dic, index=[0])\n",
    "        df = pd.concat([df, df_temp])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month December\n",
      "/wiki/Wikipedia:Articles_for_deletion/Log/2014_December_31\n",
      "Articles to be crawled in this page:  39.0\n",
      "/wiki/Wikipedia:Articles_for_deletion/Log/2014_December_30\n",
      "Articles to be crawled in this page:  85.0\n",
      "/wiki/Wikipedia:Articles_for_deletion/Log/2014_December_29\n",
      "Articles to be crawled in this page:  37.0\n",
      "/wiki/Wikipedia:Articles_for_deletion/Log/2014_December_28\n",
      "Articles to be crawled in this page:  41.0\n",
      "/wiki/Wikipedia:Articles_for_deletion/Log/2014_December_27\n",
      "Articles to be crawled in this page:  65.0\n",
      "/wiki/Wikipedia:Articles_for_deletion/Log/2014_December_26\n",
      "Articles to be crawled in this page:  49.0\n",
      "/wiki/Wikipedia:Articles_for_deletion/Log/2014_December_25\n",
      "Articles to be crawled in this page:  59.0\n",
      "/wiki/Wikipedia:Articles_for_deletion/Log/2014_December_24\n",
      "Articles to be crawled in this page:  34.0\n",
      "/wiki/Wikipedia:Articles_for_deletion/Log/2014_December_23\n"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "\n",
    "seedURL = \"https://en.wikipedia.org/wiki/Wikipedia:Archived_deletion_discussions#Deletion_discussions/\"\n",
    "archivePage = requests.get(seedURL)\n",
    "soup = BeautifulSoup(archivePage.content, \"html.parser\")\n",
    "\n",
    "# Get the year\n",
    "years = []\n",
    "yearContents = []\n",
    "for yearContent in soup.find_all(\"h2\", limit=17):\n",
    "    year = yearContent.get_text().split(\"[\")[0]\n",
    "    if year == \"Contents\":\n",
    "        continue\n",
    "    years.append(year)\n",
    "    yearContents.append(yearContent)\n",
    "\n",
    "# print(years[5])\n",
    "df = pd.DataFrame()\n",
    "df = crawl_year(years[5], yearContents[5], df)\n",
    "\n",
    "elapsedTime = timeit.default_timer() - startTime\n",
    "print(\"Crawl time \", elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = df.to_csv (r'2014.csv', index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Css - binder",
   "language": "python",
   "name": "cssresearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
